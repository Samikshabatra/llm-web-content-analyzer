{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9970ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\LLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_20548\\2550544180.py:6: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from scraper import search_content \n",
    "from IPython.display import Markdown,display\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510e535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6ced11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What is machine learning?\n",
       "Machine learning\n",
       "Welcome\n",
       "Caret right\n",
       "Introduction\n",
       "Overview\n",
       "Machine learning types\n",
       "Machine learning algorithms\n",
       "Caret right\n",
       "Data science for machine learning\n",
       "Statistical machine learning\n",
       "Linear algebra for machine learning\n",
       "Uncertainty quantification\n",
       "Bias variance tradeoff\n",
       "Bayesian Statistics\n",
       "Caret right\n",
       "Feature Engineering\n",
       "Overview\n",
       "Feature selection\n",
       "Feature extraction\n",
       "Vector embedding\n",
       "Latent space\n",
       "Caret right\n",
       "Dimensionality reduction\n",
       "Principal component analysis\n",
       "Linear discriminant analysis\n",
       "Upsampling\n",
       "Downsampling\n",
       "Synthetic data\n",
       "Data leakage\n",
       "Caret right\n",
       "Supervised learning\n",
       "Overview\n",
       "Caret right\n",
       "Regression\n",
       "Linear regression\n",
       "Lasso regression\n",
       "Ridge regression\n",
       "State space model\n",
       "Time series\n",
       "Autoregressive model\n",
       "Caret right\n",
       "Classification\n",
       "Overview\n",
       "Decision trees\n",
       "K-nearest neighbors (KNNs)\n",
       "Naive bayes\n",
       "Random forest\n",
       "Support vector machine\n",
       "Logistic regression\n",
       "Caret right\n",
       "Ensemble learning\n",
       "Overview\n",
       "Boosting\n",
       "Bagging\n",
       "Gradient boosting\n",
       "Gradient boosting classifier\n",
       "Caret right\n",
       "Self-supervised learning\n",
       "Overview\n",
       "Transfer learning\n",
       "Caret right\n",
       "Unsupervised learning\n",
       "Overview\n",
       "Caret right\n",
       "Clustering\n",
       "Overview\n",
       "K means clustering\n",
       "Hierarchical clustering\n",
       "A priori algorithm\n",
       "Gaussian mixture model\n",
       "Anomaly detection\n",
       "Caret right\n",
       "Semi-supervised learning\n",
       "Overview\n",
       "Caret right\n",
       "Recommendation engine\n",
       "Collaborative filtering\n",
       "Content based filtering\n",
       "Caret right\n",
       "Reinforcement learning\n",
       "Overview\n",
       "Reinforcement learning human feedback\n",
       "Caret right\n",
       "Deep Learning\n",
       "Overview\n",
       "Caret right\n",
       "Neural networks\n",
       "Overview\n",
       "Backpropagation\n",
       "Encoder-decoder model\n",
       "Recurrent neural networks\n",
       "Long short-term memory (LSTM)\n",
       "Convolutional neural networks\n",
       "Caret right\n",
       "Transformer models\n",
       "Overview\n",
       "Attention mechanism\n",
       "Grouped query attention\n",
       "Positional encoding\n",
       "Autoencoder\n",
       "Mamba model\n",
       "Graph neural network\n",
       "Caret right\n",
       "Generative AI\n",
       "Overview\n",
       "Generative model\n",
       "Generative AI vs. predictive AI\n",
       "Caret right\n",
       "Large language models (LLMs)\n",
       "Overview\n",
       "Reasoning models\n",
       "Small language models\n",
       "Instruction tuning\n",
       "LLM parameters\n",
       "LLM temperature\n",
       "LLM benchmarks\n",
       "LLM customization\n",
       "Caret right\n",
       "AI image generation\n",
       "Diffusion models\n",
       "Variational autoencoder (VAE)\n",
       "Generative adversarial networks (GANs)\n",
       "Caret right\n",
       "Multimodal AI\n",
       "Overview\n",
       "Vision language models\n",
       "Tutorial: Build an AI stylist\n",
       "Tutorial: Multimodal AI queries using Llama\n",
       "Tutorial: Multimodal AI queries using Pixtral\n",
       "Tutorial: Automatic podcast transcription with Granite\n",
       "Tutorial: PPT AI image analysis answering system\n",
       "Caret right\n",
       "Retrieval augmented generation (RAG)\n",
       "Overview\n",
       "GraphRAG\n",
       "Tutorial: Build a multimodal RAG system with Docling and Granite\n",
       "Tutorial: Evaluate RAG pipline using Ragas\n",
       "Tutorial: RAG chunking strategies\n",
       "Tutorial: Graph RAG using knowledge graphs\n",
       "Tutorial: Inference scaling to improve multimodal RAG\n",
       "Caret right\n",
       "AI code generation\n",
       "Overview\n",
       "Vibe coding\n",
       "Caret right\n",
       "AI agents\n",
       "Visit the 2025 Guide to AI Agents\n",
       "Caret right\n",
       "Model training\n",
       "Overview\n",
       "Loss function\n",
       "Training data\n",
       "Model parameters\n",
       "Caret right\n",
       "Optimization algorithm\n",
       "Gradient descent\n",
       "Stochastic gradient descent\n",
       "Caret right\n",
       "Model hyperparameters\n",
       "Hyperparameter tuning\n",
       "Learning rate\n",
       "Caret right\n",
       "Fine tuning\n",
       "Overview\n",
       "Parameter efficient fine tuning (PEFT)\n",
       "LoRA\n",
       "Tutorial: Fine tuning Granite model with LoRA\n",
       "Regularization\n",
       "Foundation models\n",
       "Overfitting\n",
       "Underfitting\n",
       "Caret right\n",
       "N-shot learning\n",
       "Few shot learning\n",
       "Zero shot learning\n",
       "Knowledge distillation\n",
       "Meta learning\n",
       "Data augmentation\n",
       "Caret right\n",
       "Continual learning\n",
       "Catastrophic forgetting\n",
       "Caret right\n",
       "Machine learning libraries\n",
       "Overview\n",
       "Scikit-learn\n",
       "XGboost\n",
       "PyTorch\n",
       "Caret right\n",
       "MLOps\n",
       "Overview\n",
       "AI lifecyle\n",
       "AI inference\n",
       "Model deployment\n",
       "Machine learning pipeline\n",
       "Data labeling\n",
       "Caret right\n",
       "Model governance\n",
       "Model risk management\n",
       "Model drift\n",
       "AutoML\n",
       "Model selection\n",
       "Federated learning\n",
       "Distributed machine learning\n",
       "AI stack\n",
       "Caret right\n",
       "Natural language processing\n",
       "Overview\n",
       "Natural language understanding\n",
       "Caret right\n",
       "Text classification\n",
       "Overview\n",
       "Sentiment analysis\n",
       "Tutorial: Spam text classifier with PyTorch\n",
       "Machine translation\n",
       "Caret right\n",
       "Text mining\n",
       "Overview\n",
       "Information retrieval\n",
       "Information extraction\n",
       "Topic modeling\n",
       "Latent semantic analysis\n",
       "Latent Dirichlet Allocation\n",
       "Named entity recognition\n",
       "Word embeddings\n",
       "Bag of words\n",
       "Intelligent search\n",
       "Speech recognition\n",
       "Stemming and lemmatization\n",
       "Text summarization\n",
       "Conversational AI\n",
       "Conversational analytics\n",
       "Natural language generation\n",
       "Caret right\n",
       "Computer vision\n",
       "Overview\n",
       "Image classification\n",
       "Object detection\n",
       "Caret right\n",
       "Image segmentation\n",
       "Instance segmentation\n",
       "Semantic segmentation\n",
       "Optical character recognition\n",
       "Image recognition\n",
       "Visual inspection\n",
       "Author\n",
       "Dave Bergmann\n",
       "Senior Staff Writer, AI Models\n",
       "IBM Think\n",
       "What is machine learning?\n",
       "Machine learning is the subset of artificial intelligence (AI) focused on algorithms that can “learn” the patterns of training data and, subsequently, make accurate\n",
       "inferences\n",
       "about new data. This pattern recognition ability enables machine learning models to make decisions or predictions without explicit, hard-coded instructions.\n",
       "Machine learning has come to dominate the field of AI: it provides the backbone of most modern AI systems, from\n",
       "forecasting\n",
       "models to autonomous vehicles to\n",
       "large language models (LLMs)\n",
       "and other\n",
       "generative AI\n",
       "tools.\n",
       "The central premise of machine learning (ML) is that if you optimize a model’s performance on a dataset of tasks that adequately resemble the real-world problems it will be used for—through a process called\n",
       "model training\n",
       "—the model can make accurate predictions on the new data it sees in its ultimate use case.\n",
       "Training itself is simply a means to an end: generalization, the translation of strong performance on training data to useful results in real-world scenarios, is the fundamental goal of machine learning. In essence, a trained model is applying patterns it learned from training data to infer the correct output for a real-world task: the deployment of an\n",
       "AI model\n",
       "is therefore called\n",
       "AI inference\n",
       ".\n",
       "Deep learning\n",
       ", the subset of machine learning driven by large—or rather, “deep”—\n",
       "artificial neural networks\n",
       ", has emerged over the past few decades as the state-of-the-art AI model architecture across nearly every domain in which AI is used. In contrast to the explicitly defined algorithms of traditional machine learning, deep learning relies on distributed “networks” of mathematical operations that provide an unparalleled ability to learn the intricate nuances of very complex data. Because deep learning requires very large amounts of data and computational resources, its advent has coincided with the escalated importance\n",
       "“big data”\n",
       "and\n",
       "graphics processing units (GPUs).\n",
       "The discipline of machine learning is closely intertwined with that of\n",
       "data science\n",
       ". In a sense, machine learning can be understood as a collection of algorithms and techniques to automate data analysis and (more importantly) apply learnings from that analysis to the autonomous execution of relevant tasks.\n",
       "The origin of the term (albeit not the core concept itself) is often attributed to Arthur L. Samuel’s 1959 article in IBM Journal, “Some Studies in Machine Learning Using the Game of Checkers.” In the paper’s introduction, Samuel neatly articulates machine learning’s ideal outcome: “a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program.”\n",
       "1\n",
       "Machine learning vs. artificial intelligence\n",
       "Though “machine learning” and “artificial intelligence” are often used interchangeably, they are not quite synonymous. In short:\n",
       "all machine learning is AI, but not all AI is machine learning\n",
       ".\n",
       "In the popular imagination, “AI” is usually associated with science fiction—typically through depictions of what’s more properly called\n",
       "artificial general intelligence (AGI)\n",
       ", like HAL 9000 in\n",
       "2001: A Space Odyssey\n",
       "or Ava in\n",
       "Ex Machina\n",
       "—or, more recently, with\n",
       "generative\n",
       "AI\n",
       ". But “artificial intelligence” is a catch-all term for any program that can use information to make decisions or predictions without active human involvement.\n",
       "The most elementary AI systems are a series of if-then-else statements, with rules and logic programmed explicitly by a data scientist.  At the simplest level, even a rudimentary thermostat is a rules-based AI system: when programmed with simple rules like\n",
       "IF room_temperature < 67, THEN turn_on_heater\n",
       "and\n",
       "IF room_temperature > 72, THEN turn_on_air_conditioner\n",
       "the thermostat is capable of autonomous decision-making without further human intervention. At a more complex level, a large and intricate rules-based\n",
       "decision tree\n",
       "programmed by medical experts could parse symptoms, circumstances and comorbidities to aid diagnosis or prognosis.\n",
       "2\n",
       "Unlike in expert systems, the logic by which a machine learning model operates isn’t explicitly programmed—it’s\n",
       "learned\n",
       "through experience. Consider a program that filters email spam: rules-based AI requires a data scientist to manually devise accurate, universal criteria for spam; machine learning requires only the selection of an appropriate algorithm and an adequate dataset of sample emails. In training, the model is shown sample emails and predicts which are spam; the error of its predictions is calculated, and its algorithm is adjusted to reduce error; this process is repeated until the model is accurate. The newly trained ML model has\n",
       "implicitly\n",
       "learned how to identify spam.\n",
       "As the tasks an AI system is to perform become more complex, rules-based models become increasingly brittle: it’s often impossible to explicitly define every pattern and variable a model must consider. Machine learning systems have emerged as the dominant mode of artificial intelligence because implicit learning patterns from the data itself is inherently more flexible, scalable and accessible.\n",
       "Think Newsletter\n",
       "Join over 100,000 subscribers who read the latest news in tech\n",
       "Stay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. See the\n",
       "IBM Privacy Statement\n",
       ".\n",
       "Thank you! You are subscribed.\n",
       "Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe\n",
       "here\n",
       ". Refer to our\n",
       "IBM Privacy Statement\n",
       "for more information.\n",
       "https://www.ibm.com/us-en/privacy\n",
       "How machine learning works\n",
       "Machine learning works through mathematical logic. The relevant characteristics (or \"features\") of each data point must therefore be expressed numerically, so that the data itself can be fed into a mathematical algorithm that will \"learn\" to map a given input to the desired output.\n",
       "Data points in machine learning are usually represented in vector form, in which each element (or dimension) of a data point’s\n",
       "vector embedding\n",
       "corresponds to its numerical value for a specific feature. For data modalities that are inherently numerical, such as financial data or geospatial coordinates, this is relatively straightforward. But many data modalities, such as text, images, social media graph data or app user behaviors, are not inherently numerical, and therefore entail less immediately intuitive feature engineering to be expressed in an ML-ready way.\n",
       "The (often manual) process of choosing which aspects of data to use in machine learning algorithms is called\n",
       "feature selection\n",
       ".\n",
       "Feature extraction\n",
       "techniques refine data down to only its most relevant, meaningful dimensions. Both are subsets of\n",
       "feature engineering\n",
       ", the broader discipline of preprocessing raw data for use in machine learning. One notable distinction of deep learning is that it typically operates on raw data and automates much of the feature engineering—or at least the feature extraction—process. This makes deep learning more scalable, albeit less interpretable, than traditional machine learning.\n",
       "Machine learning model parameters and optimization\n",
       "For a practical example, consider a simple\n",
       "linear regression\n",
       "algorithm for predicting home sale prices based on a weighted combination of three variables: square footage, age of house and number of bedrooms. Each house is represented as a vector embedding with 3 dimensions:\n",
       "[square footage, bedrooms, age]\n",
       ". A 30-year-old house with 4 bedrooms and 1900 square feet could be represented as\n",
       "[1900, 4, 30]\n",
       "(though for mathematical purposes those numbers might first be scaled, or normalized, to a more uniform range).\n",
       "The algorithm is a straightforward mathematical function:\n",
       "Price = (A * square footage) + (B * number of rooms) – (C * Age) + Base Price\n",
       "Here,\n",
       "A\n",
       ",\n",
       "B\n",
       "and\n",
       "C\n",
       "are the model parameters: adjusting them will adjust how heavily the model weighs each variable. The goal of machine learning is to find the optimal values for such model parameters: in other words, the parameter values that result in the overall function outputting the most accurate results. While most real-world instances of machine learning involve more complex algorithms with a greater number of input variables, the principle remains the same: optimizing the algorithm's adjustable parameters to yield greater accuracy.\n",
       "Types of machine learning\n",
       "All machine learning methods can be categorized as one of three distinct learning paradigms: supervised learning, unsupervised learning or reinforcement learning, based on the nature of their training objectives and (often but not always) by the type of training data they entail.\n",
       "Supervised learning\n",
       "trains a model to predict the “correct” output for a given input. It applies to tasks that require some degree of accuracy relative to some external “\n",
       "ground truth,\n",
       "” such as classification or regression.\n",
       "Unsupervised learning\n",
       "trains a model to discern intrinsic patterns, dependencies and correlations in data. Unlike in supervised learning, unsupervised learning tasks don’t involve any external ground truth against which its outputs should be compared.\n",
       "Reinforcement learning (RL)\n",
       "trains a model to evaluate its environment and take an action that will garner the greatest reward. RL scenarios don’t entail the existence of a singular ground truth, but they do entail the existence of “good” and “bad” (or neutral) actions.\n",
       "The end-to-end training process for a given model can, and often does, involve hybrid approaches that leverage more than one of these learning paradigms. For instance, unsupervised learning is often used to preprocess data for use in supervised or reinforcement learning. Large language models (LLMs) typically undergo their initial training (pre-training) and\n",
       "fine-tuning\n",
       "through variants of supervised learning, followed by more fine-tuning through RL techniques such as\n",
       "reinforcement learning from human feedback (RLHF)\n",
       ".\n",
       "In a similar but distinct practice, various\n",
       "ensemble learning\n",
       "methods aggregate the outputs of multiple algorithms.\n",
       "Supervised learning\n",
       "Supervised learning algorithms train models for tasks requiring accuracy, such as\n",
       "classification\n",
       "or regression. Supervised machine learning powers both state-of-the-art deep learning models and a wide array of traditional ML models still widely employed across industries.\n",
       "Regression\n",
       "models predict continuous values, such as price, duration, temperature or size. Examples of traditional regression algorithms include\n",
       "linear regression\n",
       ", polynomial regression and\n",
       "state space models\n",
       ".\n",
       "Classification\n",
       "models predict discrete values, such as the category (or\n",
       "class\n",
       ") a data point belongs to, a binary decision or a specific action to be taken. Examples of traditional classification algorithms include\n",
       "support vector machines (SVMs)\n",
       ",\n",
       "Naïve Bayes\n",
       "and\n",
       "logistic regression\n",
       ".\n",
       "Many supervised ML algorithms can be used for either task. For instance, the output of what’s nominally a regression algorithm can subsequently be used to inform a classification prediction.\n",
       "To be measured and optimized for accuracy, a model’s outputs must be compared to a\n",
       "ground truth\n",
       ": the ideal or “correct” output for any given input. In conventional supervised learning, that ground truth is provided by labeled data. An email spam detection model is trained on a dataset of emails that have each been labeled as\n",
       "SPAM\n",
       "or\n",
       "NOT SPAM\n",
       ". An\n",
       "image segmentation\n",
       "model is trained on images in which every individual pixel has been annotated by its classification. The goal of supervised learning is to adjust the model’s parameters until its outputs consistently match the ground truth provided by those labels.\n",
       "Essential to supervised learning is the use of a\n",
       "loss function\n",
       "that measures the divergence (“loss”) between the model’s output and the ground truth across a batch of training inputs. The objective of supervised learning is defined mathematically as\n",
       "minimizing the output of a loss function\n",
       ". Once loss has been computed, various optimization algorithms—most of which involve calculating the\n",
       "derivative\n",
       "(s) of the loss function—are used to identify parameter adjustments that will reduce loss.\n",
       "Because this process traditionally requires a human in the loop to provide ground truth in the form of data annotations, it’s called “supervised” learning. As such, the use of labeled data was historically considered the definitive characteristic of supervised learning. But on the most fundamental level, the hallmark of supervised learning is the existence of some ground truth and the training objective of minimizing the output of loss function that measures divergence from it.\n",
       "To accommodate a more versatile notion of supervised learning, modern ML terminology uses “supervision” or “supervisory signals” to refer generically to any source of ground truth.\n",
       "Self-supervised learning\n",
       "Labeling data can become prohibitively costly and time-consuming for complex tasks and large datasets.\n",
       "Self-supervised learning\n",
       "entails training on tasks in which a supervisory signal is obtained directly from\n",
       "unlabeled\n",
       "data—hence “self” supervised.\n",
       "For instance,\n",
       "autoencoders\n",
       "are trained to compress (or\n",
       "encode\n",
       ") input data, then reconstruct (or\n",
       "decode\n",
       ") the original input using that compressed representation. Their training objective is to minimize\n",
       "reconstruction error\n",
       ", using the original input itself as ground truth. Self-supervised learning is also the primary training method for LLMs: models are provided text samples with certain words hidden or masked and tasked with predicting the missing words.\n",
       "Self-supervised learning is frequently associated with\n",
       "transfer learning\n",
       ", as it can provide\n",
       "foundation models\n",
       "with broad capabilities that will then be\n",
       "fine-tuned\n",
       "for more specific tasks.\n",
       "Semi-supervised learning\n",
       "Whereas self-supervised learning is essentially supervised learning on unlabeled data,\n",
       "semi-supervised learning\n",
       "methods use both labeled data and unlabeled data. Broadly speaking, semi-supervised learning comprises techniques that use information from the available labeled data to make assumptions about the unlabeled data points so that the latter can be incorporated into supervised learning workflows.\n",
       "Unsupervised learning\n",
       "Unsupervised machine learning algorithms discern intrinsic patterns in unlabeled data, such as similarities, correlations or potential groupings. They’re most useful in scenarios where such patterns aren’t necessarily apparent to human observers. Because unsupervised learning doesn’t assume the preexistence of a known “correct” output, they don’t require supervisory signals or conventional loss functions—hence “unsupervised.”\n",
       "Most unsupervised learning methods perform one of the following functions:\n",
       "Clustering\n",
       "algorithms partition unlabeled data points into “clusters,” or groupings, based on their proximity or similarity to one another. They’re typically used for tasks like market segmentation or fraud detection. Prominent clustering algorithms include K-means clustering, Gaussian mixture models (GMMs) and density-based methods such as DBSCAN.\n",
       "Association\n",
       "algorithms discern correlations, such as between a particular action and certain conditions. For instance, e-commerce businesses such as Amazon use unsupervised\n",
       "association\n",
       "models to power recommendation engines.\n",
       "Dimensionality reduction\n",
       "algorithms reduce the complexity of data points by representing them with a smaller number of features—that is, in fewer dimensions—while\n",
       "preserving their meaningful characteristics\n",
       ". They’re often used for preprocessing data, as well as for tasks such as data compression or data visualization. Prominent dimensionality reduction algorithms include\n",
       "autoencoders\n",
       ",\n",
       "principal component analysis (PCA)\n",
       ",\n",
       "linear discriminant analysis (LDA)\n",
       "and\n",
       "t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
       "As their name suggests, unsupervised learning algorithms can be broadly understood as somewhat “optimizing themselves.” For example,\n",
       "this animation\n",
       "demonstrates how a k-means clustering algorithm iteratively optimizes the centroid of each cluster on its own. The challenge of training unsupervised models therefore focuses on effective data preprocessing and properly\n",
       "tuning hyperparameters\n",
       "that influence the learning process but are not themselves learnable, such as the\n",
       "learning rate\n",
       "or number of clusters.\n",
       "Reinforcement learning (RL)\n",
       "Whereas supervised learning trains models by optimizing them to match ideal exemplars and unsupervised learning algorithms fit themselves to a dataset,\n",
       "reinforcement learning\n",
       "models are trained holistically through trial and error. They’re used prominently in robotics, video games,\n",
       "reasoning models\n",
       "and other use cases in which the space of possible solutions and approaches are particularly large, open-ended or difficult to define. In RL literature, an AI system is often referred to as an “agent.”\n",
       "Rather than the independent pairs of input-output data used in supervised learning, reinforcement learning (RL) operates on interdependent state-action-reward data tuples. Instead of minimizing error, the objective of reinforcement learning is optimizing parameters to maximize reward.\n",
       "A mathematical framework for reinforcement learning is built primarily on the following components:\n",
       "The\n",
       "state space\n",
       "contains all available information relevant to decisions that the model might make. The state typically changes with each action that the model takes.\n",
       "The\n",
       "action space\n",
       "contains all the decisions that the model is permitted to make at a moment. In a board game, for instance, the action space comprises all legal moves available at a given time. In text generation, the action space comprises the entire “vocabulary” of tokens available to an LLM.\n",
       "The\n",
       "reward signal\n",
       "is the feedback—positive or negative, typically expressed as a scalar value—provided to the agent as a result of each action. The value of the reward signal could be determined by explicit rules, by a r\n",
       "eward function\n",
       ", or by a separately trained\n",
       "reward model\n",
       ".\n",
       "A\n",
       "policy\n",
       "is the “thought process” that drives an RL agent’s behavior. Mathematically speaking, a policy (\n",
       "π\n",
       ") is a function that takes a state (\n",
       "s\n",
       ") as input and returns an action (\n",
       "a\n",
       "):   π(s)→a .\n",
       "In\n",
       "policy-based\n",
       "RL methods like proximal policy optimization (PPO), the model learns a policy directly. In\n",
       "value-based\n",
       "methods like Q-learning, the agent learns a value function that computes a score for how “good” each state is, then chooses actions that lead to higher-value states. Consider a maze: a policy-based agent might learn “at this corner, turn left,” while a value-based agent learns a score for each position and simply moves to an adjacent position with a better score. Hybrid approaches, such as actor-critic methods, learn a value function that’s then used to optimize a policy.\n",
       "In deep reinforcement learning, the policy is represented as a\n",
       "neural network\n",
       ".\n",
       "Deep learning\n",
       "Deep learning\n",
       "employs\n",
       "artificial neural networks\n",
       "with many layers—hence “deep”—rather than the explicitly designed algorithms of traditional machine learning. Though neural networks were introduced early in the history of machine learning, it wasn’t until the late 2000s and early 2010s, enabled in part by advancements in\n",
       "GPUs\n",
       ", that they became dominant in most subfields of AI.\n",
       "Loosely inspired by the human brain, neural networks comprise interconnected layers of “neurons” (or\n",
       "nodes\n",
       "), each of which performs its own mathematical operation (called an “activation function”). The output of each node’s activation function serves as input to each of the nodes of the following layer and so on until the final layer, where the network’s final output is computed. Crucially, the activation functions performed at each node are\n",
       "nonlinear\n",
       ", enabling neural networks to model complex patterns and dependencies.\n",
       "Each connection between two neurons is assigned a unique\n",
       "weight\n",
       ": a multiplier that increases or decreases one neuron’s contribution to a neuron in the following layer. These weights, along with unique\n",
       "bias\n",
       "terms added to each neuron's activation function, are the parameters to be optimized through machine learning.\n",
       "The\n",
       "backpropagation\n",
       "algorithm enables the computation of how\n",
       "each individual node\n",
       "contributes to the overall output of the loss function, allowing even millions or billions of model weights to be individually optimized through\n",
       "gradient descent\n",
       "algorithms. Because of the volume and granularity of updates required to achieve optimal results, deep learning requires very large amounts of data and computational resources compared to traditional ML.\n",
       "That distributed structure affords deep learning models their incredible power and versatility. Imagine training data as data points scattered on a 2-dimensional graph. Essentially, traditional machine learning aims to find a single curve that runs through every one of those data points; deep learning pieces together an arbitrary number of smaller, individually adjustable lines to form the desired shape. Neural networks are\n",
       "universal approximators\n",
       ": it has been theoretically proven that for any function, there exists a neural network arrangement that can reproduce it.\n",
       "3, 4\n",
       "Having said that, just because something is\n",
       "theoretically\n",
       "possible doesn’t mean it’s practically achievable through existing training methods. For many years, adequate performance on certain tasks remained out of reach even for deep learning models—but over time, modifications to the standard neural network architecture have unlocked new capabilities for ML models.\n",
       "Convolutional neural networks (CNNs)\n",
       "Convolutional neural networks (CNNs)\n",
       "add\n",
       "convolutional layers\n",
       "to neural networks. In mathematics, a convolution is an operation where one function modifies (or\n",
       "convolves\n",
       ") the shape of another. In CNNs, convolutional layers are used to extract important features from data by\n",
       "applying weighted “filters”.\n",
       "CNNs are primarily associated with computer vision models and image data, but have a number of other important use cases.\n",
       "Recurrent neural networks (RNNs)\n",
       "Recurrent neural networks (RNNs)\n",
       "are designed to work on sequential data. Whereas conventional feedforward neural networks map a single input to a single output, RNNs map a\n",
       "sequence\n",
       "of inputs to an output by operating in a recurrent loop in which the output for a given step in the input sequence serves as input to the computation for the following step. In effect this creates an internal “memory,” called the\n",
       "hidden state\n",
       ", that allows RNNs to understand context and order.\n",
       "Transformers\n",
       "Transformer models\n",
       ", first introduced in 2017, are largely responsible for the advent of LLMs and other pillars of generative AI, achieving state-of-the-art results across most subdomains of machine learning. Like RNNs, transformers are ostensibly designed for sequential data, but clever workarounds have enabled most data modalities to be processed by transformers. The unique strength of transformer models comes from their innovative\n",
       "attention mechanism\n",
       ", which enables the models to selectively focus on the parts of the input data most relevant at a specific moment in a sequence.\n",
       "Mamba models\n",
       "Mamba models\n",
       "are a relatively new neural network architecture, first introduced in 2023, based on a unique variation of\n",
       "state space models (SSMs)\n",
       ". Like transformers, Mamba models provide an innovative means of selectively prioritizing the most relevant information at a given moment. Mamba has recently emerged as a rival to the transformer architecture, particularly for LLMs.\n",
       "Machine learning use cases\n",
       "Most applications of machine learning fall into one or more of the following categories, which are defined primarily by their use cases and the data modalities they operate upon.\n",
       "Computer vision\n",
       "Computer vision is the subdomain of AI concerned with image data, video data other data modalities that require a model or machine to “see,” from healthcare diagnostics to facial recognition to self-driving cars. Notable subfields of computer vision include image classification,\n",
       "object detection\n",
       ",\n",
       "image segmentation\n",
       "and\n",
       "optical character recognition (OCR\n",
       ").\n",
       "Natural language processing (NLP)\n",
       "The field of\n",
       "natural language processing (NLP)\n",
       "spans a diverse array of tasks concerning text, speech and other language data. Notable subdomains of NLP include\n",
       "chatbots\n",
       ",\n",
       "speech recognition\n",
       ",\n",
       "language translation,\n",
       "sentiment analysis\n",
       ",\n",
       "text generation\n",
       ",\n",
       "summarization\n",
       "and\n",
       "AI agents\n",
       ". In modern NLP, large language models continue to advance the state of the art at an unprecedented pace.\n",
       "Time series analysis\n",
       "Time series models\n",
       "are applied anomaly detection, market analysis and related pattern recognition or prediction tasks. They use machine learning on historical data for a variety of forecasting use cases.\n",
       "Image generation\n",
       "Diffusion models\n",
       ",\n",
       "variational autoencoders (VAEs)\n",
       "and\n",
       "generative adversarial networks (GANs)\n",
       "can be used to generate original images that apply pixel patterns learned from training data.\n",
       "Mixture of Experts | 19 December, episode 86\n",
       "Decoding AI: Weekly News Roundup\n",
       "Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.\n",
       "Watch all episodes of Mixture of Experts\n",
       "Machine learning operations (MLOps)\n",
       "Machine learning operations (MLOps)\n",
       "is a set of practices for implementing an assembly line approach to building, deploying and maintaining machine learning models.\n",
       "Careful curation and preprocessing of training data, as well as appropriate\n",
       "model selection\n",
       ", are crucial steps in the MLOps pipeline. Thoughtful post-training validation, from the design of benchmark datasets to the prioritization of particular performance\n",
       "metrics\n",
       ", is necessary to ensure that a model generalizes well (and isn’t just\n",
       "overfitting\n",
       "the training data).\n",
       "Following deployment, models must be monitored for\n",
       "model drift\n",
       ", inference efficiency issues and other adverse developments. A well-defined practice of\n",
       "model governance\n",
       "is essential to continued efficacy, especially in regulated or fast-changing industries.\n",
       "Machine learning libraries\n",
       "A number of open source tools, libraries and frameworks exist for building, training and testing machine learning projects. While such libraries offer an array of pre-configured modules and abstractions to streamline the process of building ML-based models and workflows, practitioners will need to familiarize themselves with commonly used programming languages—particularly\n",
       "Python\n",
       "—to make full use of them.\n",
       "Prominent open source libraries, particularly for building deep learning models, include\n",
       "PyTorch\n",
       ",\n",
       "TensorFlow\n",
       ", Keras and the Hugging Face Transformers library.\n",
       "Notable open source\n",
       "machine learning libraries\n",
       "and toolkits focused on traditional ML include Pandas,\n",
       "Scikit-learn\n",
       ",\n",
       "XGBoost\n",
       ",\n",
       "Matplotlib\n",
       ", SciPy and NumPy among many others.\n",
       "IBM itself maintains and updates a significant library of\n",
       "tutorials\n",
       "for beginners and advanced ML practitioners alike.\n",
       "Link copied\n",
       "Report\n",
       "IBM is named a Leader in Data Science & Machine Learning\n",
       "Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.\n",
       "Read the report\n",
       "Resources\n",
       "Report\n",
       "The 2025 CEO’s guide: 5 mindshifts to supercharge business growth\n",
       "Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.\n",
       "Read the report\n",
       "Training\n",
       "Level up your ML expertise\n",
       "Learn fundamental concepts and build your skills with hands-on labs, courses, guided projects, trials and more.\n",
       "Explore ML courses\n",
       "Ebook\n",
       "Unlock the power of generative AI + ML\n",
       "Learn how to confidently incorporate generative AI and machine learning into your business.\n",
       "Read the ebook\n",
       "Guide\n",
       "Put AI to work: Driving ROI with gen AI\n",
       "Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions.\n",
       "Read the guide\n",
       "Ebook\n",
       "How to choose the right foundation model\n",
       "Learn how to select the most suitable AI foundation model for your use case.\n",
       "Read the ebook\n",
       "AI models\n",
       "Explore IBM Granite\n",
       "IBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.\n",
       "Meet Granite\n",
       "Guide\n",
       "How to thrive in this new era of AI with trust and confidence\n",
       "Dive into the 3 critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.\n",
       "Read the guide\n",
       "Report\n",
       "AI in Action Report\n",
       "We surveyed 2,000 organizations about their AI initiatives to discover what's working, what's not and how you can get ahead.\n",
       "Read the report\n",
       "Related solutions\n",
       "IBM® watsonx Orchestrate™\n",
       "Easily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes with IBM® watsonx Orchestrate™.\n",
       "Explore watsonx Orchestrate\n",
       "AI for developers\n",
       "Move your applications from prototype to production with the help of our AI development solutions.\n",
       "Explore AI development tools\n",
       "AI consulting and services\n",
       "Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\n",
       "Explore AI services\n",
       "Take the next step\n",
       "Whether you choose to customize pre-built apps and skills or build and deploy custom agentic services using an AI studio, the IBM watsonx platform has you covered.\n",
       "Explore watsonx Orchestrate\n",
       "Explore watsonx.ai\n",
       "Footnotes\n",
       "All links reside outside ibm.com\n",
       "1.\n",
       "\"Some Studies in Machine Learning Using the Game of Checkers\n",
       ",\"\n",
       "IBM Journal\n",
       "(accessed through MIT), 3 July 1959\n",
       "2.\n",
       "\"Using Decision Trees as an Expert System for Clinical Decision Support for COVID-19,\n",
       "\"\n",
       "Interactive Journal of Medical Research, Vol 12\n",
       ", 30 January 2023\n",
       "3.\n",
       "\"Kolmogorov's Mapping Neural Network Existence Theorem,\n",
       "\"\n",
       "Proceedings of the IEEE First International Conference on Neural Networks\n",
       "(accessed through University of Waterloo)\n",
       ",\n",
       "1987\n",
       "4.\n",
       "\"Multilayer Feedforward Networks with a Non-Polynomial Activation Function Can Approximate Any Function,\n",
       "\"\n",
       "Center for Research on Information Systems (New York University), March 1992"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://www.ibm.com/think/topics/machine-learning\"\n",
    "content = search_content(url)\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81e7575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-flash-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d200ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a summary of the provided content in clear bullet points, aiming for at least 15-20 distinct and meaningful aspects:\n",
       "\n",
       "*   **Definition of Machine Learning:** ML is a subset of AI where algorithms learn patterns from training data to make predictions or decisions on new, unseen data without explicit programming.\n",
       "*   **Core Principle of ML:** The fundamental goal is generalization, meaning a model's strong performance on training data translates to useful results in real-world scenarios.\n",
       "*   **Model Training:** This is the process of optimizing a model's performance on a dataset that resembles real-world problems, enabling it to learn patterns.\n",
       "*   **AI Inference:** This refers to the deployment of a trained AI model, where it applies learned patterns to infer correct outputs for real-world tasks.\n",
       "*   **Machine Learning Types Overview:** The content outlines various categories of machine learning, including supervised, unsupervised, semi-supervised, self-supervised, and reinforcement learning.\n",
       "*   **Supervised Learning:** This type involves training models on labeled data to learn a mapping from inputs to outputs, commonly used for regression and classification.\n",
       "*   **Unsupervised Learning:** This approach trains models on unlabeled data to discover hidden patterns, structures, or relationships, often used for clustering and anomaly detection.\n",
       "*   **Semi-Supervised Learning:** A hybrid approach that utilizes both labeled and unlabeled data for training, offering benefits when labeling is scarce.\n",
       "*   **Self-Supervised Learning:** A form of unsupervised learning where labels are automatically generated from the data itself, enabling learning from large unlabeled datasets.\n",
       "*   **Reinforcement Learning:** This paradigm involves training agents through trial and error, receiving rewards or penalties for their actions in an environment.\n",
       "*   **Deep Learning:** A subfield of ML characterized by the use of deep artificial neural networks with multiple layers to learn complex patterns.\n",
       "*   **Neural Networks:** The foundational architecture for deep learning, comprising interconnected nodes (neurons) organized in layers, learning through processes like backpropagation.\n",
       "*   **Transformer Models:** Advanced neural network architectures, particularly impactful in NLP and computer vision, utilizing attention mechanisms for enhanced context understanding.\n",
       "*   **Generative AI:** A rapidly growing area focused on creating new content (text, images, audio, etc.) rather than just predicting or classifying existing data.\n",
       "*   **Large Language Models (LLMs):** Powerful generative AI models trained on vast amounts of text data, capable of understanding, generating, and reasoning with human language.\n",
       "*   **AI Image Generation:** Techniques like Diffusion Models, VAEs, and GANs are used to create new, realistic images based on textual prompts or existing data.\n",
       "*   **Multimodal AI:** AI systems that can process and understand information from multiple types of data simultaneously (e.g., text, images, audio), leading to richer insights.\n",
       "*   **Retrieval Augmented Generation (RAG):** A technique that combines generative models with external knowledge retrieval to produce more accurate and contextually relevant responses.\n",
       "*   **Model Training Aspects:** Key components include loss functions to measure errors, training data to learn from, and model parameters that are adjusted during training.\n",
       "*   **Optimization Algorithms:** Methods like gradient descent and stochastic gradient descent are used to minimize the loss function and find optimal model parameters.\n",
       "*   **Hyperparameter Tuning:** The process of adjusting model hyperparameters (e.g., learning rate) to improve performance, distinct from model parameters learned during training.\n",
       "*   **Fine-Tuning:** Adapting a pre-trained model to a specific task or dataset, often involving adjusting a subset of its parameters.\n",
       "*   **Regularization Techniques:** Methods employed to prevent overfitting, such as L1 and L2 regularization, ensuring models generalize better.\n",
       "*   **Overfitting and Underfitting:** Common issues where a model performs too well on training data (overfitting) or not well enough (underfitting), hindering generalization.\n",
       "*   **Learning Paradigms:** Concepts like N-shot, few-shot, and zero-shot learning describe how models perform with varying amounts of task-specific training data.\n",
       "*   **Continual Learning:** The ability of a model to learn new information over time without forgetting previously acquired knowledge, addressing catastrophic forgetting.\n",
       "*   **Machine Learning Libraries:** Essential tools like Scikit-learn, XGBoost, and PyTorch provide frameworks and algorithms for building and deploying ML models.\n",
       "*   **MLOps (Machine Learning Operations):** The practice of applying DevOps principles to machine learning, focusing on streamlining the ML lifecycle from development to deployment and monitoring.\n",
       "*   **Model Governance and Risk Management:** Essential for ensuring responsible and ethical AI deployment, addressing issues like model drift and bias.\n",
       "*   **Natural Language Processing (NLP):** A field of AI enabling computers to understand, interpret, and generate human language, encompassing tasks like text classification and machine translation.\n",
       "*   **Computer Vision:** A field of AI that enables computers to \"see\" and interpret images and videos, covering tasks like image classification and object detection."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize the following content in clear bullet points atleast 15-20.\n",
    "Each bullet point should explain a distinct, meaningful aspect.\n",
    "The bullets together should give a comprehensive overview.\n",
    "\n",
    "Content:\n",
    "{content[:6000]}\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "response = model.generate_content(prompt)\n",
    "display(Markdown(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31565485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the best suited and most important keywords to understand the context of the provided content:\n",
       "\n",
       "**Core Concepts:**\n",
       "\n",
       "*   **Machine Learning:** The central theme.\n",
       "*   **Deep Learning:** A major subfield and focus.\n",
       "*   **Generative AI:** A significant and prominent topic.\n",
       "*   **AI Agents:** A newly highlighted area.\n",
       "*   **Model Training:** Fundamental to all ML.\n",
       "*   **Natural Language Processing (NLP):** A key application area.\n",
       "*   **MLOps:** Essential for productionizing ML.\n",
       "\n",
       "**Key Sub-fields & Techniques:**\n",
       "\n",
       "*   **Supervised Learning:** A primary learning paradigm.\n",
       "*   **Unsupervised Learning:** Another primary learning paradigm.\n",
       "*   **Reinforcement Learning:** A distinct learning paradigm.\n",
       "*   **Self-supervised Learning:** A growing area.\n",
       "*   **Semi-supervised Learning:** Bridging supervised and unsupervised.\n",
       "*   **Ensemble Learning:** Combining multiple models.\n",
       "*   **Neural Networks:** The building blocks of deep learning.\n",
       "*   **Transformer Models:** Dominant architecture in modern AI.\n",
       "*   **Large Language Models (LLMs):** A highly significant generative AI application.\n",
       "*   **Recommendation Engine:** A common practical application.\n",
       "*   **Feature Engineering:** Crucial for data preparation.\n",
       "*   **Dimensionality Reduction:** Important for data handling.\n",
       "*   **Uncertainty Quantification:** A critical aspect of model reliability.\n",
       "\n",
       "**Important Algorithms/Models (Examples):**\n",
       "\n",
       "*   **Linear Regression**\n",
       "*   **Logistic Regression**\n",
       "*   **Decision Trees**\n",
       "*   **Random Forest**\n",
       "*   **Support Vector Machine (SVM)**\n",
       "*   **K-Means Clustering**\n",
       "*   **Recurrent Neural Networks (RNNs)**\n",
       "*   **Long Short-Term Memory (LSTM)**\n",
       "*   **Convolutional Neural Networks (CNNs)**\n",
       "*   **Generative Adversarial Networks (GANs)**\n",
       "*   **Diffusion Models**\n",
       "\n",
       "**Crucial Related Concepts:**\n",
       "\n",
       "*   **Data Science:** The broader field.\n",
       "*   **Feature Engineering:** Data preparation.\n",
       "*   **Hyperparameter Tuning:** Model optimization.\n",
       "*   **Fine Tuning:** Adapting pre-trained models.\n",
       "*   **Regularization:** Preventing overfitting.\n",
       "*   **Overfitting/Underfitting:** Common training problems.\n",
       "*   **Loss Function:** Objective for training.\n",
       "*   **Optimization Algorithm:** How models learn.\n",
       "\n",
       "**Emerging & Specific Applications:**\n",
       "\n",
       "*   **Multimodal AI:** Combining different data types.\n",
       "*   **Retrieval Augmented Generation (RAG):** Enhancing generative models.\n",
       "*   **AI Image Generation:** A popular generative AI task.\n",
       "*   **AI Code Generation:** A growing application.\n",
       "\n",
       "These keywords provide a comprehensive overview of the topics covered, from fundamental machine learning principles to advanced deep learning architectures, generative AI applications, and the operational aspects of deploying ML systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Find the keywords of the following content..\n",
    "Give the best suited and important keywords to understand the context of the content..\n",
    "\n",
    "Content:\n",
    "{content[:4000]}\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1c1b39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ❓ Question"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "List the types of Machine Learning"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ✅ Answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Supervised learning\n",
       "- Self-supervised learning\n",
       "- Unsupervised learning\n",
       "- Semi-supervised learning\n",
       "- Reinforcement learning"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = input(\"Enter your question: \")\n",
    "prompt = f\"\"\"\n",
    "You are a domain-aware assistant.\n",
    "\n",
    "RULES (STRICT):\n",
    "- Answer the question ONLY using the information present in the CONTEXT.\n",
    "- Do NOT use outside knowledge.\n",
    "- Do NOT guess or infer.\n",
    "- If the answer is not explicitly present, respond exactly with:\n",
    "  \"Answer not found in the provided content.\"\n",
    "\n",
    "CONTEXT:\n",
    "{content[:6000]}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "answer = model.generate_content(prompt)\n",
    "display(Markdown(\"## ❓ Question\"))\n",
    "display(Markdown(question))\n",
    "display(Markdown(\"## ✅ Answer\"))\n",
    "display(Markdown(answer.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75126c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 🔍 Evidence Grounding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Supervised learning\n",
       "- Self-supervised learning\n",
       "- Unsupervised learning\n",
       "- Semi-supervised learning\n",
       "- Reinforcement learning"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evidence_prompt = f\"\"\"\n",
    "From the CONTEXT below, extract the exact sentences\n",
    "that support the ANSWER.\n",
    "\n",
    "If no sentence supports it, say \"Not grounded\".\n",
    "\n",
    "CONTEXT:\n",
    "{content[:3000]}\n",
    "\n",
    "ANSWER:\n",
    "{answer.text}\n",
    "\"\"\"\n",
    "\n",
    "evidence = model.generate_content(evidence_prompt)\n",
    "\n",
    "display(Markdown(\"## 🔍 Evidence Grounding\"))\n",
    "display(Markdown(evidence.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f32bd18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 📊 Confidence Score"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**80 / 100**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confidence_prompt = f\"\"\"\n",
    "Evaluate how well the ANSWER is supported by the CONTEXT.\n",
    "\n",
    "Scoring Guidelines:\n",
    "- 0–30 → Weak or hallucinated (not supported by context)\n",
    "- 31–60 → Partially supported\n",
    "- 61–85 → Mostly supported\n",
    "- 86–100 → Fully grounded and faithful\n",
    "\n",
    "Respond with ONLY a single integer number between 0 and 100.\n",
    "No explanation.\n",
    "\n",
    "CONTEXT:\n",
    "{content[:3000]}\n",
    "\n",
    "ANSWER:\n",
    "{answer.text}\n",
    "\"\"\"\n",
    "\n",
    "confidence = model.generate_content(confidence_prompt)\n",
    "\n",
    "display(Markdown(\"### 📊 Confidence Score\"))\n",
    "display(Markdown(f\"**{confidence.text.strip()} / 100**\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
